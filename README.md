# Attribute-Conditioned-GAN
A GAN-based framework for attribute-conditioned cartoon face generation using metadata and reinforcement learning. The model combines a DCGAN, an attribute classifier, and an RL agent to control latent codes, enabling diverse and semantically guided image synthesis under limited computational resources.

## Implementation Overview

### Goal and scope
- This project implements a lightweight GAN-based pipeline designed to run on Google Colab (T4) with limited runtime.
- To reduce I/O and training cost, only a small subset of the dataset is indexed and used (2 subfolders × 500 samples each).

### Data layout and metadata parsing
- Each subfolder contains PNG images and a corresponding CSV file per image.
- Each CSV stores rows in the form `(attribute_name, value, cardinality)`.
- During indexing:
  - A unified index file is created containing image paths and selected attributes.
  - Each attribute value is mapped to a contiguous class index, which is required for classification and RL conditioning.

### Preprocessing and DataLoader
- Images are resized to **64×64** and normalized to **[-1, 1]**, matching the Generator’s `tanh` output.
- The DataLoader uses:
  - `num_workers = 2`
  - `pin_memory = True`
  - `persistent_workers = True`
  to improve throughput on Colab.

### Model components
- **Generator (netG)**  
  DCGAN-style architecture using transposed convolutions.  
  Maps a latent vector \( z \in \mathbb{R}^{100} \) to a 64×64 RGB image in the range [-1, 1].

- **Discriminator (netD)**  
  DCGAN-style convolutional network that classifies images as real or fake using a sigmoid output.

- **Attribute classifier (netC)**  
  A lightweight CNN with shared feature extraction and multiple classification heads  
  (one per attribute), trained only on real images.

- **RL agent (agent)**  
  A small MLP that takes a target attribute vector (as floats) and outputs a latent code \( z \).  
  This enables attribute-controlled image generation.

### Training flow (two stages)

#### Stage 1: GAN + attribute classifier training
- Train **Discriminator** using BCE loss on:
  - real images (with label smoothing)
  - fake images generated by the Generator
- Train **Generator** to fool the Discriminator.
- Train **Attribute Classifier** in parallel using cross-entropy over all attribute heads.
- Saved checkpoints:
  - `generator_pretrained.pth`
  - `discriminator_frozen.pth`
  - `classifier_frozen.pth`

#### Stage 2: RL agent training (G / D / C frozen)
- Freeze Generator, Discriminator, and Attribute Classifier.
- Train only the RL agent.
- For each episode:
  - Sample target attribute indices.
  - Agent produces latent vector `z`.
  - Generator produces an image from `z`.
  - Compute:
    - **Quality reward** from Discriminator output.
    - **Metadata loss** from classifier predictions vs target attributes.
- Optimize the agent using a weighted combination of:
  - negative quality reward  
  - metadata classification loss
- Save trained agent as `agent_final.pth`.

### Evaluation and outputs
- **FID**:
  - Computed using InceptionV3 (2048-D features).
  - Real and generated samples are embedded and compared using the Fréchet distance.
- **LPIPS**:
  - Computed using a pretrained LPIPS network (e.g., AlexNet).
  - Measures perceptual distance between real–fake image pairs.
- **Image generation**:
  - Generated samples are saved to a folder named after `student_id`
  - Files follow the pattern: `student_id-XXXX.png`

### Practical limitations (current setup)
- The small training subset and limited number of epochs lead to high FID and visually noisy outputs.
- The RL agent only affects generation when its latent output is used; sampling random noise reflects GAN-only behavior.
